{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bf1685-01a7-4e66-a38a-b6ba0d3f4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from Bio.PDB import PDBParser, MMCIFParser, PPBuilder, is_aa\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "731bcc95-b9ba-4f44-9e6e-eef2f809163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load + clean + save\n",
    "file = r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\06012025_4-da_SoL_25uM_on-bead_jurkat_PSMs.txt\"\n",
    "cleaned_file = file.replace(\".txt\", \"_dotHeaders.txt\")\n",
    "\n",
    "df = pd.read_csv(file, sep=\"\\t\")\n",
    "df.columns = df.columns.str.replace(\" \", \".\", regex=False)\n",
    "df.to_csv(cleaned_file, sep=\"\\t\", index=False)\n",
    "\n",
    "udata = pd.read_csv(cleaned_file, sep=\"\\t\")\n",
    "\n",
    "train_set = pd.read_csv(\n",
    "    r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Louis\\SOL Processing\\SoL-7-8_merged_PSMs_040722.txt\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "human_proteome = pd.read_excel(r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\Public_data\\human_proteome_2025.xlsx\")\n",
    "human_proteome.columns = human_proteome.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64dbfb1-c965-462c-9c0e-c691072d7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labeled(uData, modIDs):\n",
    "    mod_regex = '|'.join(modIDs)\n",
    "    return uData[uData['Modifications'].str.contains(mod_regex, na=False)].copy()\n",
    "\n",
    "def process_psms_tmt(uData, modIDs, train_set, ptmRS=True, quant_level=\"MS3\"):\n",
    "    base_cols = [\n",
    "        \"Confidence\", \"PSM.Ambiguity\", \"Annotated.Sequence\", \"Modifications\", \"Master.Protein.Accessions\",\n",
    "        \"Protein.Accessions\", \"Number.of.Missed.Cleavages\", \"Charge\", \"Delta.Score\", \"Delta.Cn\",\n",
    "        \"mz.in.Da\", \"MHplus.in.Da\", \"Theo.MHplus.in.Da\", \"Delta.M.in.ppm\", \"Delta.mz.in.Da\",\n",
    "        \"Isolation.Interference.in.Percent\", \"Average.Reporter.SN\", \"Ion.Inject.Time.in.ms\",\n",
    "        \"RT.in.min\", \"First.Scan\", \"Spectrum.File\", \"File.ID\", \"Quan.Info\", \"XCorr\"\n",
    "    ]\n",
    "\n",
    "    tmt10 = [\n",
    "        \"Abundance.126\", \"Abundance.127N\", \"Abundance.127C\", \"Abundance.128N\", \"Abundance.128C\",\n",
    "        \"Abundance.129N\", \"Abundance.129C\", \"Abundance.130N\", \"Abundance.130C\", \"Abundance.131\"\n",
    "    ]\n",
    "    base_cols += tmt10\n",
    "\n",
    "    if quant_level == \"MS3\":\n",
    "        base_cols.append(\"SPS.Mass.Matches.in.Percent\")\n",
    "    if ptmRS and \"ptmRS.Best.Site.Probabilities\" in uData.columns:\n",
    "        base_cols.append(\"ptmRS.Best.Site.Probabilities\")\n",
    "\n",
    "    uData = uData[base_cols].copy()\n",
    "\n",
    "    # 🧬 Clean protein accessions\n",
    "    uData['Master.Protein.Accessions'] = uData['Master.Protein.Accessions'].replace(\"\", np.nan)\n",
    "    uData['Master.Protein.Accessions'] = uData['Master.Protein.Accessions'].fillna(uData['Protein.Accessions'])\n",
    "    uData['Master.Protein.Accessions'] = uData['Master.Protein.Accessions'].str.split(\";\").str[0]\n",
    "    uData[\"Prot_totalPSMs\"] = uData.groupby(\"Master.Protein.Accessions\")[\"Master.Protein.Accessions\"].transform(\"count\")\n",
    "\n",
    "    # 🧬 Extract core sequence\n",
    "    uData['Upper_Seq'] = uData['Annotated.Sequence'].str.split(r'\\.', expand=True)[1].str.upper()\n",
    "\n",
    "    # 🧼 Clean mods\n",
    "    mods = uData['Modifications'].str.replace(\";\", \"\", regex=False).str.replace(\" \", \"\", regex=False)\n",
    "    mods = mods.str.replace(r'M\\d*\\(Oxidation\\)', '', regex=True)\n",
    "    mods = mods.str.replace(r'C\\d*\\(Carbamidomethyl\\)', '', regex=True)\n",
    "\n",
    "    mod_pattern = r'[A-Z]\\d+\\([^\\)]+\\)'\n",
    "    mod_sites = mods.str.findall(mod_pattern)\n",
    "\n",
    "    # Retain TMT mods ✅\n",
    "    uData['label_sites'] = mod_sites\n",
    "    uData = uData.explode(\"label_sites\").reset_index(drop=True)\n",
    "\n",
    "    uData[\"label_site\"] = uData[\"label_sites\"]\n",
    "    uData[\"label_AA\"] = uData[\"label_site\"].str[0]\n",
    "    uData[\"label_loc\"] = uData[\"label_site\"].str.extract(r'([0-9]+)')\n",
    "    uData.drop(columns=[\"label_sites\"], inplace=True)\n",
    "\n",
    "    for mod in modIDs:\n",
    "        uData[mod] = uData['Modifications'].str.contains(mod, na=False).astype(int)\n",
    "\n",
    "    uData['scanID'] = uData['First.Scan'].astype(str) + \"_\" + uData['File.ID'].astype(str)\n",
    "    uData['scanID_pep'] = uData['scanID'] + \"_\" + uData['Annotated.Sequence'].str.upper()\n",
    "    uData['uniqueID'] = uData['scanID_pep'] + \"_\" + uData['Modifications']\n",
    "\n",
    "    uData['pepLength'] = uData['Upper_Seq'].str.len()\n",
    "    uData['numTMT'] = uData['Modifications'].str.count(\"TMT\")\n",
    "    uData['pepLength_numTMT'] = uData['pepLength'].astype(str) + \"_\" + uData['numTMT'].astype(str)\n",
    "\n",
    "    # 🧪 Labeling\n",
    "    labeled = extract_labeled(uData, modIDs)\n",
    "    labeled['labeled'] = 1\n",
    "    uData['labeled'] = uData['uniqueID'].isin(labeled['uniqueID']).astype(int)\n",
    "    unlabeled = uData[uData['labeled'] == 0].copy()\n",
    "\n",
    "    # 📊 Features\n",
    "    uData['numPSMs'] = uData.groupby('scanID')['scanID'].transform('count')\n",
    "    uData['agreePSMs'] = uData.groupby('scanID_pep')['scanID_pep'].transform('count') / uData['numPSMs']\n",
    "    uData['scoreDiff'] = uData['Delta.Score'].fillna(uData['Delta.Cn'])\n",
    "\n",
    "    rt_avg_upl = unlabeled.groupby(\"pepLength_numTMT\")[\"RT.in.min\"].mean().rename(\"RTavg_fromUPL\")\n",
    "    rt_avg_lpl = labeled.groupby(\"pepLength_numTMT\")[\"RT.in.min\"].mean().rename(\"RTavg_fromLPL\")\n",
    "    rt_means = pd.concat([rt_avg_upl, rt_avg_lpl], axis=1).reset_index()\n",
    "    uData = uData.merge(rt_means, on=\"pepLength_numTMT\", how=\"left\")\n",
    "    uData['RT_Diff_fromUPL'] = uData['RTavg_fromUPL'] - uData['RT.in.min']\n",
    "\n",
    "    # 🚫 Drop invalids\n",
    "    scale_cols = [\"numPSMs\", \"scoreDiff\", \"RT_Diff_fromUPL\", \"agreePSMs\"]\n",
    "    train_set = train_set.dropna(subset=scale_cols).copy()\n",
    "    uData = uData.dropna(subset=scale_cols).copy()\n",
    "\n",
    "    # ⚙️ Normalize\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_set[scale_cols])\n",
    "    train_scaled = scaler.transform(train_set[scale_cols])\n",
    "    test_scaled = scaler.transform(uData[scale_cols])\n",
    "\n",
    "    model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "    model.fit(train_scaled, train_set['labeled'])\n",
    "\n",
    "    probs_train = model.predict_proba(train_scaled)[:, 1]\n",
    "    probs_test = model.predict_proba(test_scaled)[:, 1]\n",
    "    preds_train = np.where(probs_train > 0.5, \"Labeled\", \"Unlabeled\")\n",
    "    preds_test = np.where(probs_test > 0.5, \"Labeled\", \"Unlabeled\")\n",
    "\n",
    "    # Output formatting\n",
    "    train_out = train_set.copy()\n",
    "    train_out[\"glm.probs\"] = probs_train\n",
    "    train_out[\"glm.pred\"] = preds_train\n",
    "    train_out[\"model_set\"] = \"train\"\n",
    "\n",
    "    test_out = uData.copy()\n",
    "    test_out[\"glm.probs\"] = probs_test\n",
    "    test_out[\"glm.pred\"] = preds_test\n",
    "    test_out[\"model_set\"] = \"test\"\n",
    "    test_out[\"label_confidence\"] = np.where(\n",
    "        test_out[\"glm.probs\"] >= 0.85, \"High\",\n",
    "        np.where(test_out[\"glm.probs\"] >= 0.5, \"Medium\", \"Low\")\n",
    "    )\n",
    "\n",
    "    return test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5575779-5d4c-4170-9154-de9944305229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_sort_sites(s):\n",
    "    if not s: return []\n",
    "    sites = s.split(\";\")\n",
    "    filtered = [re.sub(r'\\([^\\)]+\\)', '', x) for x in sites if \"TMT6plex\" not in x]\n",
    "    freq = pd.Series(filtered).value_counts().to_dict()\n",
    "    sorted_sites = sorted(set(filtered), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "    return sorted_sites, freq\n",
    "\n",
    "def process_peps_tmt(processedPSMs, modIDs, plex=\"10plex\", human_proteome=None):\n",
    "    processedPSMs = processedPSMs.copy()\n",
    "\n",
    "    # 🔬 Sort for top PSM per scan\n",
    "    processedPSMs_sorted = processedPSMs.sort_values(\n",
    "        by=[\"scanID\", \"Prot_totalPSMs\", \"XCorr\"], ascending=[True, False, False]\n",
    "    )\n",
    "    uniqueScans = processedPSMs_sorted.drop_duplicates(\"scanID\").copy()\n",
    "\n",
    "    # 🧠 Construct peptide labels\n",
    "    pep_label_series = uniqueScans[\"Upper_Seq\"].astype(str)\n",
    "    for mod in modIDs:\n",
    "        pep_label_series += \"_\" + uniqueScans[mod].astype(str)\n",
    "    uniqueScans[\"pep_label\"] = pep_label_series\n",
    "\n",
    "    # 📋 Basic metadata\n",
    "    pepDF = uniqueScans[[\"pep_label\", \"Master.Protein.Accessions\"]].rename(\n",
    "        columns={\"pep_label\": \"Unique_ID\", \"Master.Protein.Accessions\": \"Uniprot_ID\"}\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # 🔢 Spectra count\n",
    "    spectra_count = uniqueScans[\"pep_label\"].value_counts().rename(\"#_Spectra\").reset_index()\n",
    "    spectra_count.columns = [\"Unique_ID\", \"#_Spectra\"]\n",
    "    pepDF = pepDF.merge(spectra_count, on=\"Unique_ID\", how=\"left\")\n",
    "\n",
    "    # 🎯 Apply labels to all PSMs\n",
    "    processedPSMs[\"pep_label\"] = processedPSMs[\"Upper_Seq\"].astype(str)\n",
    "    for mod in modIDs:\n",
    "        processedPSMs[\"pep_label\"] += \"_\" + processedPSMs[mod].astype(str)\n",
    "\n",
    "    # 💰 TMT data\n",
    "    tmt10 = [\n",
    "        \"Abundance.126\", \"Abundance.127N\", \"Abundance.127C\", \"Abundance.128N\", \"Abundance.128C\",\n",
    "        \"Abundance.129N\", \"Abundance.129C\", \"Abundance.130N\", \"Abundance.130C\", \"Abundance.131\"\n",
    "    ]\n",
    "    tmt_cols = tmt10 if plex == \"10plex\" else []\n",
    "    if tmt_cols:\n",
    "        tmtData = uniqueScans[tmt_cols + [\"pep_label\"]].copy()\n",
    "        tmtData[tmt_cols] = tmtData[tmt_cols].fillna(1)\n",
    "        summedTMT = tmtData.groupby(\"pep_label\")[tmt_cols].sum().reset_index()\n",
    "        summedTMT = summedTMT.rename(columns={\"pep_label\": \"Unique_ID\"})\n",
    "        pepDF = pepDF.merge(summedTMT, on=\"Unique_ID\", how=\"left\")\n",
    "\n",
    "    # 📈 Averages\n",
    "    features_to_avg = [\"glm.probs\", \"XCorr\", \"scoreDiff\", \"numPSMs\", \"RT_Diff_fromUPL\"]\n",
    "    avg_feats = processedPSMs.groupby(\"pep_label\")[features_to_avg].mean().reset_index()\n",
    "    avg_feats = avg_feats.rename(columns={\"pep_label\": \"Unique_ID\"})\n",
    "    pepDF = pepDF.merge(avg_feats, on=\"Unique_ID\", how=\"left\")\n",
    "\n",
    "    # 🔝 Max probability\n",
    "    max_glm = processedPSMs.groupby(\"pep_label\")[\"glm.probs\"].max().reset_index()\n",
    "    max_glm.columns = [\"Unique_ID\", \"max.glm.probs\"]\n",
    "    pepDF = pepDF.merge(max_glm, on=\"Unique_ID\", how=\"left\")\n",
    "\n",
    "    # 🧬 Label site analysis\n",
    "    label_data = processedPSMs[[\"pep_label\", \"label_site\"]].dropna()\n",
    "    site_lists = label_data.groupby(\"pep_label\")[\"label_site\"].apply(lambda x: \";\".join(x)).reset_index()\n",
    "    site_lists = site_lists.rename(columns={\"pep_label\": \"Unique_ID\", \"label_site\": \"site_string\"})\n",
    "\n",
    "    def process_site_string(s):\n",
    "        sorted_sites, freq_map = clean_and_sort_sites(s)\n",
    "        aas = [site[0] for site in sorted_sites]\n",
    "        locs = [re.search(r'\\d+', site).group() for site in sorted_sites]\n",
    "        site_psms = [str(freq_map[site]) for site in sorted_sites]\n",
    "        return pd.Series({\n",
    "            \"sites\": \";\".join(sorted_sites),\n",
    "            \"aas\": \";\".join(aas),\n",
    "            \"locs\": \";\".join(locs),\n",
    "            \"site_psms\": \";\".join(site_psms),\n",
    "            \"#_PSMs\": sum(map(int, site_psms))\n",
    "        })\n",
    "\n",
    "    site_details = site_lists[\"site_string\"].apply(process_site_string)\n",
    "    site_summary = pd.concat([site_lists[\"Unique_ID\"], site_details], axis=1)\n",
    "    pepDF = pepDF.merge(site_summary, on=\"Unique_ID\", how=\"left\")\n",
    "\n",
    "    # 🧪 Extract core peptide sequence\n",
    "    pepDF[\"PEPTIDE.SEQUENCE\"] = pepDF[\"Unique_ID\"].str.extract(r\"^(.+?)_\")\n",
    "\n",
    "    if human_proteome is not None:\n",
    "        # 🔁 Map gene and sequence\n",
    "        prot_for_seq = human_proteome.rename(columns={\n",
    "            \"Gene Names (primary)\": \"Gene name\",\n",
    "            \"Sequence\": \"Protein_Sequence\"\n",
    "        })\n",
    "\n",
    "        pepDF = pepDF.merge(prot_for_seq[[\"Entry\", \"Gene name\", \"Protein_Sequence\"]],\n",
    "                            left_on=\"Uniprot_ID\", right_on=\"Entry\", how=\"left\")\n",
    "\n",
    "        # 🧬 New: TARG_PEPRANGE logic\n",
    "        def build_peptide_range(row):\n",
    "            pep = row[\"PEPTIDE.SEQUENCE\"]\n",
    "            full_seq = row[\"Protein_Sequence\"]\n",
    "            uid = row[\"Uniprot_ID\"]\n",
    "            if pd.isna(pep) or pd.isna(full_seq) or pd.isna(uid): return \"\"\n",
    "            start = full_seq.find(pep)\n",
    "            if start == -1:\n",
    "                return \"\"\n",
    "            end = start + len(pep)\n",
    "            return f\"{uid}_{start+1}_{end}\"\n",
    "\n",
    "        pepDF[\"TARG_PEPRANGE\"] = pepDF.apply(build_peptide_range, axis=1)\n",
    "        pepDF.drop(columns=[\"Entry\", \"Protein_Sequence\"], inplace=True)\n",
    "\n",
    "        # 🔬 Structural & functional info\n",
    "        feature_cols = [\n",
    "            \"Protein names\", \"Sequence\", \"Active site\", \"Binding site\", \"Catalytic activity\",\n",
    "            \"Cofactor\", \"DNA binding\", \"Site\", \"PDB\", \"Alphafold\", \"ChEMBL\"\n",
    "        ]\n",
    "        missing = [col for col in feature_cols if col not in human_proteome.columns]\n",
    "        if missing:\n",
    "            print(f\"⚠️ Missing columns: {missing}\")\n",
    "        else:\n",
    "            pepDF = pepDF.merge(human_proteome[[\"Entry\"] + feature_cols],\n",
    "                                left_on=\"Uniprot_ID\", right_on=\"Entry\", how=\"left\")\n",
    "            pepDF.drop(columns=[\"Entry\"], inplace=True)\n",
    "\n",
    "    # 🧬 Fix Unique_ID format\n",
    "    if \"Gene name\" in pepDF.columns:\n",
    "        pepDF[\"Unique_ID\"] = pepDF[\"Unique_ID\"].str.replace(r\"_\\d+$\", \"\", regex=True) + \"_\" + pepDF[\"Gene name\"].fillna(\"NA\")\n",
    "\n",
    "    # 🧼 Finalize\n",
    "    pepDF.fillna(\"\", inplace=True)\n",
    "\n",
    "    # 📦 Order columns\n",
    "    cols = list(pepDF.columns)\n",
    "    if \"#_PSMs\" in cols and \"#_Spectra\" in cols:\n",
    "        cols.remove(\"#_PSMs\")\n",
    "        cols.insert(cols.index(\"#_Spectra\") + 1, \"#_PSMs\")\n",
    "    if \"Gene name\" in cols and \"Uniprot_ID\" in cols:\n",
    "        cols.remove(\"Gene name\")\n",
    "        cols.insert(cols.index(\"Uniprot_ID\") + 1, \"Gene name\")\n",
    "\n",
    "    return pepDF[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b68545-c7c8-4c36-8fdd-29c3543875b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 🧬 Define modification keyword(s) and export a cleaned PSM file\n",
    "\n",
    "modIDs = [\"DAL\"]  # matches any mod column containing \"DAL\"\n",
    "\n",
    "# 🔁 Filter training data\n",
    "train_set_filtered = train_set[\n",
    "    (train_set[\"paired\"] == 1) | (train_set[\"labeled\"] == 0)\n",
    "]\n",
    "\n",
    "processed_psms = process_psms_tmt(\n",
    "    udata,\n",
    "    modIDs,\n",
    "    train_set_filtered,\n",
    "    ptmRS=True,\n",
    "    quant_level=\"MS3\"\n",
    ")\n",
    "\n",
    "processed_psms_filtered = processed_psms[\n",
    "    (processed_psms[\"labeled\"] == 1) &\n",
    "    (processed_psms[\"glm.probs\"] >= 0.5)\n",
    "]\n",
    "\n",
    "processed_peps = process_peps_tmt(\n",
    "    processed_psms_filtered,\n",
    "    modIDs,\n",
    "    plex=\"10plex\",\n",
    "    human_proteome=human_proteome\n",
    ")\n",
    "\n",
    "processed_peps.to_excel(\n",
    "    r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\06012025_4-da_SoL_25uM_on-bead_jurkat_PSMs_processed.xlsx\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01676453-53fe-46db-8afc-3a59dbc98793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural proteomics, step 1: snatching PDB and Alphaphold that contains the peptide sequence.\n",
    "pdb_dir = r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\Public_data\\pdb_cache\"\n",
    "os.makedirs(pdb_dir, exist_ok=True)\n",
    "\n",
    "# 🧬 Initialize parsers\n",
    "pdb_parser = PDBParser(QUIET=True)\n",
    "cif_parser = MMCIFParser(QUIET=True)\n",
    "\n",
    "def extract_chain_sequences(structure):\n",
    "    ppb = PPBuilder()\n",
    "    chain_seqs = {}\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            peptides = ppb.build_peptides(chain)\n",
    "            seq = ''.join(str(p.get_sequence()) for p in peptides)\n",
    "            if seq:\n",
    "                chain_seqs[chain.id] = seq\n",
    "    return chain_seqs\n",
    "\n",
    "def parse_structure(file_path):\n",
    "    try:\n",
    "        if file_path.endswith(\".pdb\"):\n",
    "            return pdb_parser.get_structure(\"pdb\", file_path)\n",
    "        elif file_path.endswith(\".cif\"):\n",
    "            return cif_parser.get_structure(\"cif\", file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Parsing error for {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def format_alphafold_id(raw_id):\n",
    "    \"\"\"Clean and format raw AlphaFold ID (e.g., 'O75935;') to 'AF-O75935-F1-model_v4'\"\"\"\n",
    "    return f\"AF-{str(raw_id).strip().replace(';', '')}-F1-model_v4\"\n",
    "\n",
    "def download_pdb_or_cif_or_alphafold(pdb_id, pdb_dir, alphafold_id=None):\n",
    "    pdb_id = pdb_id.lower().strip()\n",
    "    pdb_path = os.path.join(pdb_dir, f\"{pdb_id}.pdb\")\n",
    "    cif_path = os.path.join(pdb_dir, f\"{pdb_id}.cif\")\n",
    "\n",
    "    af_path = None\n",
    "    full_af_id = None\n",
    "    if alphafold_id:\n",
    "        full_af_id = format_alphafold_id(alphafold_id)\n",
    "        af_path = os.path.join(pdb_dir, f\"{full_af_id}.pdb\")\n",
    "\n",
    "    if os.path.exists(pdb_path): return pdb_path\n",
    "    if os.path.exists(cif_path): return cif_path\n",
    "    if af_path and os.path.exists(af_path): return af_path\n",
    "\n",
    "    try:\n",
    "        r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.pdb\", timeout=10)\n",
    "        if r.ok and \"ATOM\" in r.text:\n",
    "            with open(pdb_path, \"w\") as f:\n",
    "                f.write(r.text)\n",
    "            return pdb_path\n",
    "    except: pass\n",
    "\n",
    "    try:\n",
    "        r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.cif\", timeout=10)\n",
    "        if r.ok:\n",
    "            with open(cif_path, \"w\") as f:\n",
    "                f.write(r.text)\n",
    "            return cif_path\n",
    "    except: pass\n",
    "\n",
    "    if full_af_id:\n",
    "        try:\n",
    "            url = f\"https://alphafold.ebi.ac.uk/files/{full_af_id}.pdb\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.ok:\n",
    "                with open(af_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                return af_path\n",
    "        except Exception as e:\n",
    "            print(f\"❌ AlphaFold download failed for {full_af_id}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_best_pdb_mapping(df, pdb_dir):\n",
    "    df = df.copy()\n",
    "    df[\"PDB_clean\"] = \"\"\n",
    "\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        row = df.iloc[i]\n",
    "        pdb_string = str(row.get(\"PDB\", \"\"))\n",
    "        uniprot_id = row[\"Uniprot_ID\"]\n",
    "        alphafold_id = row.get(\"Alphafold\", \"\")\n",
    "\n",
    "        if len(pdb_string) > 4 and \";\" in pdb_string:\n",
    "            pep_seqs = df[df[\"Uniprot_ID\"] == uniprot_id][\"PEPTIDE.SEQUENCE\"].dropna().unique().tolist()\n",
    "            pdbs_to_check = pdb_string.split(\";\")\n",
    "            match_counts = []\n",
    "\n",
    "            for pdb_id in pdbs_to_check:\n",
    "                structure_file = download_pdb_or_cif_or_alphafold(pdb_id, pdb_dir, alphafold_id)\n",
    "                if not structure_file:\n",
    "                    match_counts.append(0)\n",
    "                    continue\n",
    "\n",
    "                structure = parse_structure(structure_file)\n",
    "                if not structure:\n",
    "                    match_counts.append(0)\n",
    "                    continue\n",
    "\n",
    "                chain_seqs = extract_chain_sequences(structure).values()\n",
    "                match_count = sum(any(pep in chain_seq for chain_seq in chain_seqs) for pep in pep_seqs)\n",
    "                match_counts.append(match_count)\n",
    "\n",
    "            if match_counts:\n",
    "                max_count = max(match_counts)\n",
    "                best_pdbs = [pdbs_to_check[j] for j, count in enumerate(match_counts) if count == max_count]\n",
    "                df.loc[df[\"Uniprot_ID\"] == uniprot_id, \"PDB_clean\"] = \";\".join(best_pdbs)\n",
    "\n",
    "            i += len(pep_seqs)\n",
    "\n",
    "        else:\n",
    "            if not pdb_string or pdb_string in [\"0\", \"nan\", \"None\"]:\n",
    "                if pd.notna(alphafold_id) and alphafold_id.strip():\n",
    "                    df.at[i, \"PDB_clean\"] = format_alphafold_id(alphafold_id)\n",
    "                else:\n",
    "                    df.at[i, \"PDB_clean\"] = \"\"\n",
    "            else:\n",
    "                df.at[i, \"PDB_clean\"] = pdb_string\n",
    "            i += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# 🧬 Input any processed file\n",
    "excel_path = r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\05152025_6-da_SoL_25uM_on-bead_jurkat_sup_fasta_PSMs_processed.xlsx\"\n",
    "sol = pd.read_excel(excel_path)\n",
    "\n",
    "# 🚀 Run mapping\n",
    "sol_with_pdb_clean = get_best_pdb_mapping(sol, pdb_dir)\n",
    "\n",
    "# 💾 Save output\n",
    "sol_with_pdb_clean.to_csv(\n",
    "    r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\05152025_6-da_SoL_25uM_on-bead_jurkat_sup_fasta_PSMs_w_PDB_filter.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fae5f94d-1792-471e-8c79-ce0ff858ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural proteomics, step 2: calculating the distance between the labeled peptides and \"active sites\" using PDB, or Alphafold\n",
    "\n",
    "# === 📁 SETUP ===\n",
    "def format_alphafold_filename(af_id):\n",
    "    \"\"\"Ensure AlphaFold ID is formatted like AF-<UniProt>-F1-model_v4\"\"\"\n",
    "    if pd.isna(af_id) or not af_id.strip(): return \"\"\n",
    "    af_id = af_id.strip().split(\";\")[0]\n",
    "    if not af_id.startswith(\"AF-\"):\n",
    "        af_id = f\"AF-{af_id}-F1-model_v4\"\n",
    "    return af_id\n",
    "\n",
    "def fetch_structure(pdb_id, cache_dir=\"pdb_cache\", alphafold_id=None):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    pdb_id = pdb_id.lower().strip()\n",
    "    alphafold_id = format_alphafold_filename(alphafold_id)\n",
    "\n",
    "    pdb_file = os.path.join(cache_dir, f\"{pdb_id}.pdb\")\n",
    "    cif_file = os.path.join(cache_dir, f\"{pdb_id}.cif\")\n",
    "    af_file = os.path.join(cache_dir, f\"{alphafold_id}.pdb\") if alphafold_id else None\n",
    "\n",
    "    if not os.path.exists(pdb_file):\n",
    "        r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.pdb\")\n",
    "        if r.ok and \"ATOM\" in r.text:\n",
    "            with open(pdb_file, \"w\") as f:\n",
    "                f.write(r.text)\n",
    "\n",
    "    if not os.path.exists(pdb_file) and not os.path.exists(cif_file):\n",
    "        r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.cif\")\n",
    "        if r.ok:\n",
    "            with open(cif_file, \"w\") as f:\n",
    "                f.write(r.text)\n",
    "\n",
    "    if not os.path.exists(pdb_file) and not os.path.exists(cif_file) and alphafold_id:\n",
    "        url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}.pdb\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.ok:\n",
    "                with open(af_file, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ AlphaFold download failed for {alphafold_id}: {e}\")\n",
    "\n",
    "    if os.path.exists(pdb_file): return pdb_file\n",
    "    if os.path.exists(cif_file): return cif_file\n",
    "    if alphafold_id and os.path.exists(af_file): return af_file\n",
    "    return None\n",
    "\n",
    "# === PDB parsing functions remain unchanged ===\n",
    "def parse_structure(path):\n",
    "    if path.endswith(\".pdb\"):\n",
    "        return PDBParser(QUIET=True).get_structure(\"pdb\", path)\n",
    "    elif path.endswith(\".cif\"):\n",
    "        return MMCIFParser(QUIET=True).get_structure(\"cif\", path)\n",
    "    return None\n",
    "\n",
    "def extract_chain_residues(structure):\n",
    "    chain_map = {}\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            coords, resnos = [], []\n",
    "            for res in chain:\n",
    "                if is_aa(res, standard=True):\n",
    "                    try:\n",
    "                        coords.append(res[\"CA\"].coord)\n",
    "                        resnos.append(res.id[1])\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if coords:\n",
    "                chain_map[chain.id] = {\"coords\": np.array(coords), \"resnos\": resnos}\n",
    "    return chain_map\n",
    "\n",
    "def extract_active_sites(active_str):\n",
    "    if pd.isna(active_str): return []\n",
    "    matches = re.findall(r\"ACTIVE\\s*(\\d+)\", active_str)\n",
    "    return [int(m) for m in matches]\n",
    "\n",
    "def find_min_distance(coords1, coords2):\n",
    "    if not coords1 or not coords2: return np.inf\n",
    "    return np.min(cdist(coords1, coords2))\n",
    "\n",
    "def measure_active_site_distance(row, cache_dir=\"pdb_cache\"):\n",
    "    pdb_id = str(row.get(\"PDB_clean\", \"\"))[:4]\n",
    "    alphafold_id = row.get(\"AlphaFold\", \"\")\n",
    "    peptide = row.get(\"PEPTIDE.SEQUENCE\", \"\")\n",
    "    targ_range = row.get(\"TARG_PEPRANGE\", \"\")\n",
    "    ref_seq = str(row.get(\"Sequence\", \"\"))\n",
    "    active_str = row.get(\"Active site\", \"\")\n",
    "\n",
    "    if not pdb_id or pdb_id == \"0\" or pdb_id.lower() in [\"nan\", \"none\"]:\n",
    "        pdb_id = alphafold_id\n",
    "\n",
    "    if any(pd.isna(x) or x == \"\" for x in [pdb_id, peptide, targ_range, ref_seq, active_str]):\n",
    "        return pd.Series([\"\", \"\", \"\", \"\", \"Missing Info\"])\n",
    "\n",
    "    start_pos = int(targ_range.split(\"_\")[1])\n",
    "    active_sites = extract_active_sites(active_str)\n",
    "    site_total = len(active_sites)\n",
    "\n",
    "    structure_file = fetch_structure(pdb_id, cache_dir, alphafold_id=alphafold_id)\n",
    "    if not structure_file:\n",
    "        return pd.Series([\"\", \"\", site_total, 0, \"Structure not found\"])\n",
    "\n",
    "    structure = parse_structure(structure_file)\n",
    "    chain_data = extract_chain_residues(structure)\n",
    "\n",
    "    min_dist = np.inf\n",
    "    max_shift = 0\n",
    "    site_in_pdb = 0\n",
    "    site_near = 0\n",
    "\n",
    "    for data in chain_data.values():\n",
    "        coords, resnos = data[\"coords\"], data[\"resnos\"]\n",
    "        try:\n",
    "            pep_start_idx = resnos.index(start_pos)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        shift = resnos[pep_start_idx] - start_pos\n",
    "        max_shift = max(max_shift, abs(shift))\n",
    "        pep_indices = range(pep_start_idx, pep_start_idx + len(peptide))\n",
    "        pep_coords = [coords[i] for i in pep_indices if i < len(coords)]\n",
    "\n",
    "        site_coords = []\n",
    "        for site in active_sites:\n",
    "            try:\n",
    "                idx = resnos.index(site + shift)\n",
    "                site_coords.append(coords[idx])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if site_coords:\n",
    "            dist = find_min_distance(pep_coords, site_coords)\n",
    "            min_dist = min(min_dist, dist)\n",
    "            site_in_pdb += len(site_coords)\n",
    "            site_near += sum(1 for s in site_coords if find_min_distance(pep_coords, [s]) <= 6)\n",
    "\n",
    "    if min_dist < np.inf:\n",
    "        return pd.Series([round(min_dist, 3), max_shift, site_total, site_in_pdb, site_near])\n",
    "    else:\n",
    "        return pd.Series([\"Not Found\", max_shift, site_total, site_in_pdb, site_near])\n",
    "\n",
    "def compute_active_site_distances(sol_df, cache_dir=\"pdb_cache\"):\n",
    "    out = sol_df.copy()\n",
    "    cols = [\"spatial_distance\", \"shift\", \"site_total\", \"site_in_pdb\", \"site_near_peptide\"]\n",
    "    out[cols] = sol_df.apply(lambda row: measure_active_site_distance(row, cache_dir), axis=1)\n",
    "    return out\n",
    "\n",
    "# === 📥 LOAD AND PROCESS ===\n",
    "excel_path = r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\04152025_2-da_SoL_25uM_2-injections_PSMs_w_PDB_filter.csv\"\n",
    "sol_with_pdb_clean = pd.read_csv(excel_path)\n",
    "\n",
    "# === 🧪 COMPUTE DISTANCES ===\n",
    "sol_with_active = compute_active_site_distances(sol_with_pdb_clean, cache_dir=\"pdb_cache\")\n",
    "sol_with_active.to_csv(\n",
    "    r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\04152025_2-da_SoL_25uM_2-injections_PSMs_w_active_site_distances.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4736a509-f40a-490b-9210-7cdfc6524339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural proteomics, step 3: calculating the distance between the labeled peptides and \"binding sites\" using PDB, or Alphafold\n",
    "# === 📁 SETUP ===\n",
    "def format_alphafold_filename(af_id):\n",
    "    \"\"\"Ensure AlphaFold ID is formatted like AF-<UniProt>-F1-model_v4\"\"\"\n",
    "    if pd.isna(af_id) or not af_id.strip(): return \"\"\n",
    "    af_id = af_id.strip().split(\";\")[0]  # remove trailing semicolon\n",
    "    if not af_id.startswith(\"AF-\"):\n",
    "        af_id = f\"AF-{af_id}-F1-model_v4\"\n",
    "    return af_id\n",
    "\n",
    "def fetch_structure(pdb_id, cache_dir=\"pdb_cache\", alphafold_id=None):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    pdb_id = pdb_id.lower().strip()\n",
    "    alphafold_id = format_alphafold_filename(alphafold_id)\n",
    "\n",
    "    pdb_file = os.path.join(cache_dir, f\"{pdb_id}.pdb\")\n",
    "    cif_file = os.path.join(cache_dir, f\"{pdb_id}.cif\")\n",
    "    af_file = os.path.join(cache_dir, f\"{alphafold_id}.pdb\") if alphafold_id else None\n",
    "\n",
    "    if not os.path.exists(pdb_file):\n",
    "        try:\n",
    "            r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.pdb\", timeout=10)\n",
    "            if r.ok and \"ATOM\" in r.text:\n",
    "                with open(pdb_file, \"w\") as f:\n",
    "                    f.write(r.text)\n",
    "        except: pass\n",
    "\n",
    "    if not os.path.exists(pdb_file) and not os.path.exists(cif_file):\n",
    "        try:\n",
    "            r = requests.get(f\"https://files.rcsb.org/download/{pdb_id.upper()}.cif\", timeout=10)\n",
    "            if r.ok:\n",
    "                with open(cif_file, \"w\") as f:\n",
    "                    f.write(r.text)\n",
    "        except: pass\n",
    "\n",
    "    if not os.path.exists(pdb_file) and not os.path.exists(cif_file) and alphafold_id:\n",
    "        try:\n",
    "            url = f\"https://alphafold.ebi.ac.uk/files/{alphafold_id}.pdb\"\n",
    "            r = requests.get(url, timeout=20)\n",
    "            if r.ok:\n",
    "                with open(af_file, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ AlphaFold download failed for {alphafold_id}: {e}\")\n",
    "\n",
    "    if os.path.exists(pdb_file): return pdb_file\n",
    "    if os.path.exists(cif_file): return cif_file\n",
    "    if alphafold_id and os.path.exists(af_file): return af_file\n",
    "    return None\n",
    "\n",
    "def parse_structure(path):\n",
    "    if path.endswith(\".pdb\"):\n",
    "        return PDBParser(QUIET=True).get_structure(\"pdb\", path)\n",
    "    elif path.endswith(\".cif\"):\n",
    "        return MMCIFParser(QUIET=True).get_structure(\"cif\", path)\n",
    "    return None\n",
    "\n",
    "# === 🧬 DATA EXTRACTORS ===\n",
    "def extract_chain_residues(structure):\n",
    "    chain_map = {}\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            coords, resnos = [], []\n",
    "            for res in chain:\n",
    "                if is_aa(res, standard=True):\n",
    "                    try:\n",
    "                        coords.append(res[\"CA\"].coord)\n",
    "                        resnos.append(res.id[1])\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            if coords:\n",
    "                chain_map[chain.id] = {\"coords\": np.array(coords), \"resnos\": resnos}\n",
    "    return chain_map\n",
    "\n",
    "def extract_binding_sites(binding_str):\n",
    "    if pd.isna(binding_str): return []\n",
    "    matches = re.findall(r\"BINDING\\s*(\\d+)\", binding_str)\n",
    "    return [int(m) for m in matches]\n",
    "\n",
    "# === ⚙️ DISTANCE COMPUTATION ===\n",
    "def find_min_distance(coords1, coords2):\n",
    "    if not coords1 or not coords2: return np.inf\n",
    "    return np.min(cdist(coords1, coords2))\n",
    "\n",
    "def measure_binding_site_distance(row, cache_dir=\"pdb_cache\"):\n",
    "    pdb_id = str(row.get(\"PDB_clean\", \"\"))[:4]\n",
    "    alphafold_id = row.get(\"AlphaFold\", \"\")\n",
    "    peptide = row.get(\"PEPTIDE.SEQUENCE\", \"\")\n",
    "    targ_range = row.get(\"TARG_PEPRANGE\", \"\")\n",
    "    ref_seq = str(row.get(\"Sequence\", \"\"))\n",
    "    binding_str = row.get(\"Binding site\", \"\")\n",
    "\n",
    "    if not pdb_id or pdb_id == \"0\" or pdb_id.lower() in [\"nan\", \"none\"]:\n",
    "        pdb_id = alphafold_id\n",
    "\n",
    "    if any(pd.isna(x) or x == \"\" for x in [pdb_id, peptide, targ_range, ref_seq, binding_str]):\n",
    "        return pd.Series([\"\", \"\", \"\", \"\", \"Missing Info\"])\n",
    "\n",
    "    try:\n",
    "        start_pos = int(targ_range.split(\"_\")[1])\n",
    "    except:\n",
    "        return pd.Series([\"\", \"\", \"\", \"\", \"Bad TARG_PEPRANGE\"])\n",
    "\n",
    "    binding_sites = extract_binding_sites(binding_str)\n",
    "    site_total = len(binding_sites)\n",
    "\n",
    "    structure_file = fetch_structure(pdb_id, cache_dir, alphafold_id=alphafold_id)\n",
    "    if not structure_file:\n",
    "        return pd.Series([\"\", \"\", site_total, 0, \"Structure not found\"])\n",
    "\n",
    "    structure = parse_structure(structure_file)\n",
    "    chain_data = extract_chain_residues(structure)\n",
    "\n",
    "    min_dist = np.inf\n",
    "    max_shift = 0\n",
    "    site_in_pdb = 0\n",
    "    site_near = 0\n",
    "\n",
    "    for data in chain_data.values():\n",
    "        coords, resnos = data[\"coords\"], data[\"resnos\"]\n",
    "        try:\n",
    "            pep_start_idx = resnos.index(start_pos)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        shift = resnos[pep_start_idx] - start_pos\n",
    "        max_shift = max(max_shift, abs(shift))\n",
    "        pep_indices = range(pep_start_idx, pep_start_idx + len(peptide))\n",
    "        pep_coords = [coords[i] for i in pep_indices if i < len(coords)]\n",
    "\n",
    "        site_coords = []\n",
    "        for site in binding_sites:\n",
    "            try:\n",
    "                idx = resnos.index(site + shift)\n",
    "                site_coords.append(coords[idx])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if site_coords:\n",
    "            dist = find_min_distance(pep_coords, site_coords)\n",
    "            min_dist = min(min_dist, dist)\n",
    "            site_in_pdb += len(site_coords)\n",
    "            site_near += sum(1 for s in site_coords if find_min_distance(pep_coords, [s]) <= 6)\n",
    "\n",
    "    if min_dist < np.inf:\n",
    "        return pd.Series([round(min_dist, 3), max_shift, site_total, site_in_pdb, site_near])\n",
    "    else:\n",
    "        return pd.Series([\"Not Found\", max_shift, site_total, site_in_pdb, site_near])\n",
    "\n",
    "# === 📊 APPLY ACROSS PEPTIDES ===\n",
    "def compute_binding_site_distances(sol_df, cache_dir=\"pdb_cache\"):\n",
    "    out = sol_df.copy()\n",
    "    cols = [\"spatial_distance\", \"shift\", \"site_total\", \"site_in_pdb\", \"site_near_peptide\"]\n",
    "    out[cols] = sol_df.apply(lambda row: measure_binding_site_distance(row, cache_dir), axis=1)\n",
    "    return out\n",
    "\n",
    "# === 📥 LOAD CSV AND PROCESS ===\n",
    "csv_path = r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\04152025_2-da_SoL_25uM_2-injections_PSMs_w_PDB_filter.csv\"\n",
    "sol_with_pdb_clean = pd.read_csv(csv_path)\n",
    "\n",
    "# === 🧪 COMPUTE AND SAVE RESULTS ===\n",
    "sol_with_binding = compute_binding_site_distances(sol_with_pdb_clean, cache_dir=\"pdb_cache\")\n",
    "sol_with_binding.to_csv(\n",
    "    r\"C:\\Users\\merce\\Scripps Research Dropbox\\Parker Lab\\Junchen\\atropoprobes\\SiteOfLabelling\\SoL_TMT_screening\\processed\\04152025_2-da_SoL_25uM_2-injections_PSMs_w_binding_site_distances.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348d562-df47-463b-aa8b-cb4f2d8b347c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
